{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c9cf36",
   "metadata": {},
   "source": [
    "# Projet Buyco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d42f7",
   "metadata": {},
   "source": [
    " ### Extraction des données sur les sites des transporteurs :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603954fc",
   "metadata": {},
   "source": [
    "Les codes qui permettent de récupérer les fichiers des sites des transporteurs ont un très grand temps d'exécution. Ils ne sont donc pas dans le notebook, mais dans 'extraction-pdf.ipynb'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a09a2",
   "metadata": {},
   "source": [
    "### Traitement des données PDF :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3a0bb",
   "metadata": {},
   "source": [
    "Tentative sans LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6846d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\lisef\\AppData\\Local\\Temp\\ipykernel_14720\\2576020930.py:5: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  myFile = 'pdfs\\ALGA.pdf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Malta', 'Napoli', 'La Spezia', 'Livorno', 'Genoa', 'Marseille', 'Alger']\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber \n",
    "\n",
    "# Pour certains PDFs, ça fonctionne\n",
    "\n",
    "myFile = 'pdfs\\ALGA.pdf'\n",
    "\n",
    "with pdfplumber.open(myFile) as pdf:\n",
    "    pages = pdf.pages\n",
    "    page = pages[-1]\n",
    "\n",
    "    Liste_pays = []\n",
    "\n",
    "    for char in page.extract_table()[1] :\n",
    "        if char != '' and char != 'To\\nFrom':\n",
    "            Liste_pays.append(char)\n",
    "\n",
    "    i = 1\n",
    "    while page.extract_table()[i][0] == '':\n",
    "        i += 1\n",
    "\n",
    "    n = len(page.extract_table())\n",
    "    for k in range(i, n):\n",
    "        if page.extract_table()[k][0] not in Liste_pays and page.extract_table()[k][0] != 'To\\nFrom':\n",
    "            Liste_pays.append(page.extract_table()[k][0])\n",
    "\n",
    "    print(Liste_pays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95331004",
   "metadata": {},
   "source": [
    "Mais pas pour tous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344219d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:13: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:13: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\lisef\\AppData\\Local\\Temp\\ipykernel_14720\\3602866473.py:13: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  myFile = f'pdfs\\{fichier}'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WEST MED', None, 'VIGO', 'SALERNO', 'TRAPANI', 'LIVORNO', 'EAST MED / RED SEA', 'MERSIN', 'GEMLIK', 'IZMIR', 'SFAX', 'SOUSSE', 'TUNIS / RADES', 'MIDDLE EAST GULF', 'JEBEL ALI', 'ASIA', 'HO CHI MINH', 'BANGKOK', 'SONGHLA', 'LAEM CHABANG', 'LATAM', 'BARRANQUILLA', 'CARTAGENA', 'GUAYAQUIL', 'MANZANILLO', 'PAITA']\n",
      "['SAN ANTONIO', 'CHANCAY', 'CALLAO', 'POSORJA', 'YOKOHAMA', 'BUSAN', 'SHANGHAI', 'NINGBO']\n",
      "['YOKOHAMA', 'BUSAN', 'SHANGHAI', 'HONG\\nKONG', 'XIAMEN', 'QINGDAO', 'NINGBO', 'CHANCAY', 'SAN ANTONIO', 'MANZANILLO', 'ENSENADA']\n",
      "['MANZANILLO', 'BALBOA', 'BUENAVENTURA', 'CALLAO', 'VALPARAISO', 'HONG KONG', 'YANTIAN', 'KAOHSIUNG', 'SHANGHAI', 'NINGBO']\n",
      "['Tanger Med', '2', '4', '8', 'Algeciras', 'Vigo', 'Leixoes', 'Lisboa']\n",
      "['BARCELONA', 'VALENCIA', 'ALGER']\n",
      "['Malta', 'Napoli', 'La Spezia', 'Livorno', 'Genoa', 'Marseille', 'Alger']\n",
      "['BARCELONA', 'MALTA', 'DJEN DJEN']\n",
      "['VALENCIA', 'MARSEILLE', 'BEJAIA']\n",
      "['VALENCIA', 'MOSTAGANEM']\n",
      "['Barcelona', 'Marseille', 'Valencia', 'Algeciras', 'Oran', 'Ghazaouet']\n",
      "['Barcelona', 'Marseille', 'Valencia', 'Algeciras', 'Oran', 'Ghazaouet']\n",
      "['Barcelona', 'Marseille', 'Valencia', 'Algeciras', 'Oran', 'Ghazaouet']\n",
      "['Valencia', '2', '5', '6', '8', '14', 'Barcelona', 'Genoa', 'La Spezia', 'Salerno']\n",
      "['Day', 'Tue', 'Mon', 'Thu', 'Sun', 'Thu', 'Port Everglades', 'Philadelphia', 'New York', 'Cartagena', 'Buenaventura']\n",
      "['New York', '9', '11', 'Baltimore', 'Charleston', 'Port Everglades']\n",
      "[None, 'Terminal', 'MAHER', 'NIT', 'GCT', 'SFCT', 'Algeciras', 'Livorno', 'Genoa', 'Fos', 'Barcelona', 'Valencia']\n",
      "[None, 'Terminal', 'MAHER', 'NIT', 'GCT', 'SFCT', 'Algeciras', 'Livorno', 'Genoa', 'Fos', 'Barcelona', 'Valencia']\n",
      "[None, 'Terminal', 'MAHER', 'NIT', 'GCT', 'SFCT', 'Algeciras', 'Livorno', 'Genoa', 'Fos', 'Barcelona', 'Valencia']\n",
      "[None, 'Terminal', 'MAHER', 'NIT', 'GCT', 'SFCT', 'Algeciras', 'Livorno', 'Genoa', 'Fos', 'Barcelona', 'Valencia']\n",
      "['Port', 'Terminal', 'Arrival', 'Departure', 'Shanghai', 'Ningbo', 'Shekou', 'Singapore', 'Port Kelang', 'Nhava Sheva', 'Karachi', 'Mundra', 'Colombo', 'Port Klang', 'Hong Kong']\n",
      "['Qingdao', '37', '43', '42', '41', '45', '46', '54', 'Shanghai', 'Ningbo', 'Nansha', 'Tanjung Pelepas', 'Singapore']\n",
      "['Mombasa', 'Nairobi', 'Mogadishu', 'Qingdao', 'Shanghai', 'Ningbo', 'Nansha', 'Singapore']\n",
      "['Mombasa', 'Nairobi', 'Mogadishu', 'Qingdao', 'Shanghai', 'Ningbo', 'Nansha', 'Singapore']\n",
      "['Mombasa', 'Nairobi', 'Mogadishu', 'Qingdao', 'Shanghai', 'Ningbo', 'Nansha', 'Singapore']\n",
      "['Qingdao', '33', '41', '42', 'Shanghai', 'Ningbo', 'Yantian', 'Singapore']\n",
      "['Bassens', '1', '2', '3', 'Montoir', 'Brest**', 'Le Havre']\n",
      "['Bassens', '1', '2', '3', 'Montoir', 'Brest**', 'Le Havre']\n",
      "['Bassens', '1', '2', '3', 'Montoir', 'Brest**', 'Le Havre']\n",
      "['ROTTERDAM', 'DUBLIN']\n",
      "['Shanghai', '39', '44', '46', '47', 'Ningbo', 'Xiamen', 'Shekou', 'Nansha', 'Singapore']\n",
      "['Shanghai', '39', '44', '46', '47', 'Ningbo', 'Xiamen', 'Shekou', 'Nansha', 'Singapore']\n",
      "['Shanghai', '39', '44', '46', '47', 'Ningbo', 'Xiamen', 'Shekou', 'Nansha', 'Singapore']\n",
      "['Nhava Sheva', '-', '1', '5', '8', 'Mundra', 'Dammam', 'Umm Qasr']\n",
      "['Nhava Sheva', '-', '1', '5', '8', 'Mundra', 'Dammam', 'Umm Qasr']\n",
      "['Nhava Sheva', '-', '1', '5', '8', 'Mundra', 'Dammam', 'Umm Qasr']\n",
      "['Nhava Sheva', '-', '1', '5', '8', 'Mundra', 'Dammam', 'Umm Qasr']\n",
      "['Ancona', 'Ravenna', 'Venezia', 'Trieste', 'Koper', 'Rijeka', 'Bar', 'Taranto', 'Malta', 'Limassol', 'Alexandria', 'Beirut', 'Lattakia', 'Tartous (**)', 'Izmir', 'Aliaga', 'Ambarli', 'Gebze', 'Gemlik']\n",
      "['Long Beach', '27', '28', '32', 'Prince Rupert']\n",
      "['From To', 'CARTAGENA', 'KINGSTON', 'VERACRUZ', 'HOUSTON', 'NEW ORLEANS', 'PARANAGUA', 'IMBITUBA', 'SANTOS', 'RIO DE JANEIRO', 'SALVADOR']\n",
      "['From To', 'CARTAGENA', 'KINGSTON', 'VERACRUZ', 'HOUSTON', 'NEW ORLEANS', 'PARANAGUA', 'IMBITUBA', 'SANTOS', 'RIO DE JANEIRO', 'SALVADOR']\n",
      "['From To', 'CARTAGENA', 'KINGSTON', 'VERACRUZ', 'HOUSTON', 'NEW ORLEANS', 'PARANAGUA', 'IMBITUBA', 'SANTOS', 'RIO DE JANEIRO', 'SALVADOR']\n",
      "['From To', 'CARTAGENA', 'KINGSTON', 'VERACRUZ', 'HOUSTON', 'NEW ORLEANS', 'PARANAGUA', 'IMBITUBA', 'SANTOS', 'RIO DE JANEIRO', 'SALVADOR']\n",
      "['Savannah', '5', '7', '9', '-', 'Kingston', 'Lafito', 'Cap Haitien']\n",
      "['Miami', '49', '51', '52', '54', '57', 'Cartagena', 'Rodman', 'Los Angeles', 'Oakland', 'Caucedo']\n",
      "['Miami', '49', '51', '52', '54', '57', 'Cartagena', 'Rodman', 'Los Angeles', 'Oakland', 'Caucedo']\n",
      "['Miami', '49', '51', '52', '54', '57', 'Cartagena', 'Rodman', 'Los Angeles', 'Oakland', 'Caucedo']\n",
      "['Port Klang', '45', '48', '50', '53', 'Haiphong', 'Yantian', 'Ningbo', 'Shanghai', 'Busan', 'Yokohama']\n",
      "['Port Klang', '45', '48', '50', '53', 'Haiphong', 'Yantian', 'Ningbo', 'Shanghai', 'Busan', 'Yokohama']\n",
      "['Port', 'Terminal (Tml)', 'Arrival', 'Departure', 'Qingdao', 'Ningbo', 'Shekou', 'Singapore', 'Khalifa Port,\\nAbu Dhabi', 'Jebel Ali', 'Hamad', 'Dammam']\n",
      "['Port', 'Terminal (Tml)', 'Arrival', 'Departure', 'Shanghai', 'Ningbo', 'Kaohsiung', 'Shekou', 'Port Klang', 'Jebel Ali', 'Umm Qasr']\n",
      "['Port', 'Terminal (Tml)', 'Arrival', 'Departure', 'Shanghai', 'Ningbo', 'Kaohsiung', 'Shekou', 'Port Klang', 'Jebel Ali', 'Umm Qasr']\n",
      "['Port', 'Terminal (Tml)', 'Arrival', 'Departure', 'Shanghai', 'Ningbo', 'Kaohsiung', 'Shekou', 'Port Klang', 'Jebel Ali', 'Umm Qasr']\n",
      "['Port', 'Terminal (Tml)', 'Arrival', 'Departure', 'Shanghai', 'Ningbo', 'Kaohsiung', 'Shekou', 'Port Klang', 'Jebel Ali', 'Umm Qasr']\n",
      "['Port', 'Terminal (Tml)', 'Arrival', 'Departure', 'Shanghai', 'Ningbo', 'Kaohsiung', 'Shekou', 'Port Klang', 'Jebel Ali', 'Umm Qasr']\n",
      "['Seattle', '26', '31', '33', '35', '37', '41', 'Vancouver']\n",
      "['Seattle', '26', '31', '33', '35', '37', '41', 'Vancouver']\n",
      "['Seattle', '26', '31', '33', '35', '37', '41', 'Vancouver']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisef\\AppData\\Local\\Temp\\ipykernel_14720\\3602866473.py:13: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  myFile = f'pdfs\\{fichier}'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m page \u001b[38;5;241m=\u001b[39m pages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     19\u001b[0m Liste_pays \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPORT\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m page\u001b[38;5;241m.\u001b[39mextract_table()[\u001b[38;5;241m1\u001b[39m] :\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m char \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m char \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFrom\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# # vérification du format \n",
    "\n",
    "# def is_html(file_path):\n",
    "#     with open(file_path, \"rb\") as f:\n",
    "#         header = f.read(1024).lower()\n",
    "#         return b\"<html\" in header or b\"<!doctype html\" in header\n",
    "\n",
    "# path='C:\\\\Users\\\\lisef\\\\projet-buyco\\\\Buyco\\\\pdfs'\n",
    "# subdirectories=os.listdir(path)\n",
    "\n",
    "\n",
    "# # boucler sur les PDFs\n",
    "\n",
    "# for fichier in subdirectories:\n",
    "#     if not is_html(f'C:\\\\Users\\\\lisef\\\\projet-buyco\\\\Buyco\\\\pdfs\\\\{fichier}'):\n",
    "#         myFile = f'pdfs\\{fichier}'\n",
    "\n",
    "#     with pdfplumber.open(myFile) as pdf:\n",
    "#         pages = pdf.pages\n",
    "#         page = pages[-1]\n",
    "\n",
    "#         Liste_pays = []\n",
    "\n",
    "#         if page.extract_table()[1][0] != 'PORT':\n",
    "#             for char in page.extract_table()[1] :\n",
    "#                 if char != '' and char != 'To\\nFrom':\n",
    "#                     Liste_pays.append(char)\n",
    "            \n",
    "#         i = 2\n",
    "#         while page.extract_table()[i][0] == '':\n",
    "#             i += 1\n",
    "\n",
    "#         n = len(page.extract_table())\n",
    "#         for k in range(i, n):\n",
    "#             if page.extract_table()[k][0] not in Liste_pays and page.extract_table()[k][0] != 'To\\nFrom':\n",
    "#                 Liste_pays.append(page.extract_table()[k][0])\n",
    "\n",
    "#         print(Liste_pays)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a76f0",
   "metadata": {},
   "source": [
    "Code final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0141d241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: 3 tables détectées\n",
      "Table 1 saved in ALGA_table_1.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber # extraire tableaus depuis fichier pdf\n",
    "import shlex # protection chaîne de caractère\n",
    "import requests # envoyer des requêtes HTTP\n",
    "import re # découper un texte à l'aide d'expressions singulières\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Conversion pdf en Markdown\n",
    "\n",
    "name_file = \"ALGA\"\n",
    "input_file = f\"pdfs/{name_file}.pdf\" \n",
    "output_file = f\"{name_file}.md\" \n",
    "\n",
    "def pdf_tables_to_markdown(pdf_path, output_md):\n",
    "    # lit un fichier pdf avec des tableaux, les extraits et les convertit en markdown pui les auvegarde dans un fichier\n",
    "    with pdfplumber.open(pdf_path) as pdf, open(output_md, \"w\", encoding=\"utf-8\") as f:\n",
    "        for page_num, page in enumerate(pdf.pages, 1):\n",
    "            tables = page.extract_tables()\n",
    "            print(f\"Page {page_num}: {len(tables)} tables détectées\")\n",
    "            for table in tables :\n",
    "                if not table or not table[0]: # vérifie que le tableau n'ets pas vide et qu'il a une en-tête\n",
    "                    continue  \n",
    "                md = \"\"\n",
    "                # En-tête Markdown\n",
    "                md += \"| \" + \" | \".join(cell if cell is not None else \"\" for cell in table[0]) + \" |\\n\" # .join(sépare les éléments de la liste par \" | \")\n",
    "                md += \"| \" + \" | \".join([\"---\"] * len(table[0])) + \" |\\n\"\n",
    "                # Lignes du tableau\n",
    "                for row in table[1:]:\n",
    "                    md += \"| \" + \" | \".join(cell if cell else \"\" for cell in row) + \" |\\n\"\n",
    "                md += \"\\n\\n\"  # Ajoute 2 lignes vides entre chaque tableau pour bien les séparer\n",
    "                f.write(md)\n",
    "\n",
    "pdf_tables_to_markdown(input_file, output_file)\n",
    "\n",
    "\n",
    "\n",
    "# Extraction des tableaux à l'aide de la plateforme de LLM\n",
    "\n",
    "input_file = f\"{name_file}.md\"\n",
    "output_file = f\"{name_file}.txt\" \n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    markdown_content = f.read()\n",
    "\n",
    "\n",
    "prompt_LLM = f\"\"\"This is a Markdown file. Extract only table and give them back to me on a readable\n",
    " .csv format. Don't comment and don't repharse. Above all else, dont add information from outside the Markdown\n",
    "file. Lets the tables how there are in the Markdown file. If a cell countains multi-line headers like To and From\n",
    "merge them into a single column header like To/From ; Markdown : {markdown_content}\"\"\" #Transit Times'\n",
    "\n",
    "safe_prompt_LLM = shlex.quote(prompt_LLM) # évite que le prompt ne soit déformé\n",
    "\n",
    "url = \"http://ollama-sam.inria.fr/api/generate\" # adresse où serveur LLM est hébergé\n",
    "data = {'model': 'mistral:7b', 'prompt': safe_prompt_LLM} # modèle utilisé, prompt\n",
    "auth = ('Bob', 'hiccup')  # utiliateur et mot de passe d'identification\n",
    "\n",
    "response = requests.post(url, json=data, auth=auth) # recupère la réponse du LLM\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    out_file.write(response.text)\n",
    "\n",
    "\n",
    "\n",
    "# Traitement du fichier .txt donné en réponse par le LLM\n",
    "\n",
    "input_file = f\"{name_file}.txt\"\n",
    "response_file = f\"{name_file}_response.txt\"\n",
    "output_base = f\"{name_file}_table\"\n",
    "\n",
    "responses = []\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as text_file:\n",
    "    for line in text_file:\n",
    "        try:\n",
    "            obj = json.loads(line.strip()) # cinvertit la chaîne JSON en dictionnaire python\n",
    "            if \"response\" in obj:\n",
    "                responses.append(obj[\"response\"])\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "full_text = \"\".join(responses) # fusionnes les réponses en un seul texte\n",
    "full_text = full_text.replace('\"\"\"', '').replace('\"', '') # retire les '''''' et \"\" pour les remplacer par ''\n",
    "\n",
    "with open(response_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(full_text)\n",
    "\n",
    "\n",
    "\n",
    "# Conversion du fichier .txt amélioré en fichier .csv\n",
    "\n",
    "raw_tables = re.split(r'\\n\\s*\\n', full_text.strip()) # sépare les différents tableaux\n",
    "\n",
    "for i, table_text in enumerate(raw_tables, 1):\n",
    "    table_text_clean = table_text.replace(\"```\", \"\").strip()\n",
    "    lines = [line.strip() for line in table_text_clean.strip().splitlines() if line.strip()]\n",
    "    output_file = f\"{output_base}_{i}.csv\"\n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "        writer = csv.writer(csv_file, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "        for line in lines:\n",
    "            writer.writerow(line.split(\",\")) # séparateur ,\n",
    "    print(f\"Table {i} saved in {output_file}\") \n",
    "\n",
    "\n",
    "# Vérification que le model LLM n'invente pas des données et fais simplement de l'extraction de données\n",
    "# Faire un test de vérification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da16f72",
   "metadata": {},
   "source": [
    "Mardown mauvais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edd8c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:14: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:14: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\lisef\\AppData\\Local\\Temp\\ipykernel_14720\\520007482.py:14: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  input_file = f\"pdfs\\{name_file}.pdf\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1 saved in GTL_0_table_1.csv\n",
      "Table 2 saved in GTL_0_table_2.csv\n",
      "Table 3 saved in GTL_0_table_3.csv\n"
     ]
    }
   ],
   "source": [
    "import shlex # protection chaîne de caractère\n",
    "import requests # envoyer des requêtes HTTP\n",
    "import re # découper un texte à l'aide d'expressions singulières\n",
    "import json\n",
    "import csv\n",
    "from markitdown import MarkItDown\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Conversion pdf en Markdown\n",
    "\n",
    "name_file = \"GTL_0\"\n",
    "input_file = f\"pdfs\\{name_file}.pdf\" \n",
    "output_file = f\"{name_file}.md\" \n",
    "\n",
    "markitdown = MarkItDown()\n",
    "\n",
    "result = markitdown.convert(input_file) #\"test.pdf\" \n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(result.text_content)\n",
    "\n",
    "\n",
    "\n",
    "# Extraction des tableaux à l'aide de la plateforme de LLM\n",
    "\n",
    "input_file = f\"{name_file}.md\"\n",
    "output_file = f\"{name_file}.txt\" \n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    markdown_content = f.read()\n",
    "\n",
    "\n",
    "prompt_LLM = f\"\"\"This is a Markdown file. Extract only table and give them back to me on a readable\n",
    " .csv format. Don't comment and don't repharse. Above all else, dont add information from outside the Markdown\n",
    "file. Lets the tables how there are in the Markdown ; Markdown : {markdown_content}\"\"\" #Transit Times'\n",
    "\n",
    "safe_prompt_LLM = shlex.quote(prompt_LLM) # évite que le prompt ne soit déformé\n",
    "\n",
    "url = \"http://ollama-sam.inria.fr/api/generate\" # adresse où serveur LLM est hébergé\n",
    "data = {'model': 'mistral:7b', 'prompt': safe_prompt_LLM} # modèle utilisé, prompt\n",
    "auth = ('Bob', 'hiccup')  # utiliateur et mot de passe d'identification\n",
    "\n",
    "response = requests.post(url, json=data, auth=auth) # recupère la réponse du LLM\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    out_file.write(response.text)\n",
    "\n",
    "\n",
    "\n",
    "# Traitement du fichier .txt donné en réponse par le LLM\n",
    "\n",
    "input_file = f\"{name_file}.txt\"\n",
    "response_file = f\"{name_file}_response.txt\"\n",
    "output_base = f\"{name_file}_table\"\n",
    "\n",
    "responses = []\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as text_file:\n",
    "    for line in text_file:\n",
    "        try:\n",
    "            obj = json.loads(line.strip()) # cinvertit la chaîne JSON en dictionnaire python\n",
    "            if \"response\" in obj:\n",
    "                responses.append(obj[\"response\"])\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "full_text = \"\".join(responses) # fusionnes les réponses en un seul texte\n",
    "full_text = full_text.replace('\"\"\"', '').replace('\"', '') # retire les '''''' et \"\" pour les remplacer par ''\n",
    "\n",
    "with open(response_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(full_text)\n",
    "\n",
    "\n",
    "\n",
    "# Conversion du fichier .txt amélioré en fichier .csv\n",
    "\n",
    "raw_tables = re.split(r'\\n\\s*\\n', full_text.strip()) # sépare les différents tableaux\n",
    "\n",
    "for i, table_text in enumerate(raw_tables, 1):\n",
    "    table_text_clean = table_text.replace(\"```\", \"\").strip()\n",
    "    lines = [line.strip() for line in table_text_clean.strip().splitlines() if line.strip()]\n",
    "    output_file = f\"{output_base}_{i}.csv\"\n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "        writer = csv.writer(csv_file, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "        for line in lines:\n",
    "            writer.writerow(line.split(\",\")) # séparateur ,\n",
    "    print(f\"Table {i} saved in {output_file}\") \n",
    "\n",
    "\n",
    "# Vérification que le model LLM n'invente pas des données et fais simplement de l'extraction de données\n",
    "# Faire un test de vérification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a583214",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import pdfplumber # extraire tableaus depuis fichier pdf\n",
    "import shlex # protection chaîne de caractère\n",
    "import requests # envoyer des requêtes HTTP\n",
    "import re # découper un texte à l'aide d'expressions singulières\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Conversion pdf en Markdown\n",
    "\n",
    "name_file = \"ALGA\"\n",
    "input_file = f\"pdfs/{name_file}.pdf\" \n",
    "output_file = f\"{name_file}.md\" \n",
    "\n",
    "def pdf_tables_to_markdown(pdf_path, output_md):\n",
    "    # lit un fichier pdf avec des tableaux, les extraits et les convertit en markdown pui les auvegarde dans un fichier\n",
    "    with pdfplumber.open(pdf_path) as pdf, open(output_md, \"w\", encoding=\"utf-8\") as f:\n",
    "        for page_num, page in enumerate(pdf.pages, 1):\n",
    "            tables = page.extract_tables()\n",
    "            print(f\"Page {page_num}: {len(tables)} tables détectées\")\n",
    "            for table in tables :\n",
    "                if not table or not table[0]: # vérifie que le tableau n'ets pas vide et qu'il a une en-tête\n",
    "                    continue  \n",
    "                md = \"\"\n",
    "                # En-tête Markdown\n",
    "                md += \"| \" + \" | \".join(cell if cell is not None else \"\" for cell in table[0]) + \" |\\n\" # .join(sépare les éléments de la liste par \" | \")\n",
    "                md += \"| \" + \" | \".join([\"---\"] * len(table[0])) + \" |\\n\"\n",
    "                # Lignes du tableau\n",
    "                for row in table[1:]:\n",
    "                    md += \"| \" + \" | \".join(cell if cell else \"\" for cell in row) + \" |\\n\"\n",
    "                md += \"\\n\\n\"  # Ajoute 2 lignes vides entre chaque tableau pour bien les séparer\n",
    "                f.write(md)\n",
    "\n",
    "pdf_tables_to_markdown(input_file, output_file)\n",
    "\n",
    "\n",
    "\n",
    "# Extraction des tableaux à l'aide de la plateforme de LLM\n",
    "\n",
    "input_file = f\"{name_file}.md\"\n",
    "output_file = f\"{name_file}.txt\" \n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    markdown_content = f.read()\n",
    "\n",
    "\n",
    "prompt_LLM = f\"\"\"This is a Markdown file. Extract only table and give them back to me on a readable\n",
    " .csv format. ; Markdown : {markdown_content}\"\"\" #Transit Times'\n",
    "\n",
    "safe_prompt_LLM = shlex.quote(prompt_LLM) # évite que le prompt ne soit déformé\n",
    "\n",
    "url = \"http://ollama-sam.inria.fr/api/generate\" # adresse où serveur LLM est hébergé\n",
    "data = {'model': 'mistral:7b', 'prompt': safe_prompt_LLM} # modèle utilisé, prompt\n",
    "auth = ('Bob', 'hiccup')  # utiliateur et mot de passe d'identification\n",
    "\n",
    "response = requests.post(url, json=data, auth=auth) # recupère la réponse du LLM\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    out_file.write(response.text)\n",
    "\n",
    "\n",
    "\n",
    "# Traitement du fichier .txt donné en réponse par le LLM\n",
    "\n",
    "input_file = f\"{name_file}.txt\"\n",
    "response_file = f\"{name_file}_response.txt\"\n",
    "output_base = f\"{name_file}_table\"\n",
    "\n",
    "responses = []\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as text_file:\n",
    "    for line in text_file:\n",
    "        try:\n",
    "            obj = json.loads(line.strip()) # cinvertit la chaîne JSON en dictionnaire python\n",
    "            if \"response\" in obj:\n",
    "                responses.append(obj[\"response\"])\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "full_text = \"\".join(responses) # fusionnes les réponses en un seul texte\n",
    "full_text = full_text.replace('\"\"\"', '').replace('\"', '') # retire les '''''' et \"\" pour les remplacer par ''\n",
    "\n",
    "with open(response_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(full_text)\n",
    "\n",
    "\n",
    "\n",
    "# Conversion du fichier .txt amélioré en fichier .csv\n",
    "\n",
    "raw_tables = re.split(r'\\n\\s*\\n', full_text.strip()) # sépare les différents tableaux\n",
    "\n",
    "for i, table_text in enumerate(raw_tables, 1):\n",
    "    table_text_clean = table_text.replace(\"```\", \"\").strip()\n",
    "    lines = [line.strip() for line in table_text_clean.strip().splitlines() if line.strip()]\n",
    "    output_file = f\"{output_base}_{i}.csv\"\n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "        writer = csv.writer(csv_file, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "        for line in lines:\n",
    "            writer.writerow(line.split(\",\")) # séparateur ,\n",
    "    print(f\"Table {i} saved in {output_file}\") \n",
    "\n",
    "\n",
    "# Vérification que le model LLM n'invente pas des données et fais simplement de l'extraction de données\n",
    "# Faire un test de vérification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b89d33",
   "metadata": {},
   "source": [
    "Une piste : garder uniquement la dernière page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2384fd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "def extract_last_page(input_path, output_path):\n",
    "    # On lit le PDF original\n",
    "    reader = PdfReader(input_path)\n",
    "    writer = PdfWriter()\n",
    "\n",
    "    # On garde la dernière page\n",
    "    last_page = reader.pages[-1]\n",
    "    writer.add_page(last_page)\n",
    "\n",
    "    # On la met dans un nouveau PDF\n",
    "    with open(output_path, \"wb\") as output_file:\n",
    "        writer.write(output_file)\n",
    "\n",
    "    print(f\"Last page saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37c1267",
   "metadata": {},
   "source": [
    "### Database :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995dc02",
   "metadata": {},
   "source": [
    "ATTENTION LE CODE NE FONCTIONNE QUE SI VOUS AVEZ UN SERVEUR LOCAL MYSQL. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b93520",
   "metadata": {},
   "source": [
    "Création de table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Table, Column, Integer, String, MetaData\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "metadata = MetaData()\n",
    "data_table = Table('data6', metadata,\n",
    "    #Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('name_carrier', String(50)),\n",
    "    Column('code_route', String(50)),\n",
    "    Column('name_route', String(100)),\n",
    "    Column('port_code', String(1000)),\n",
    "    Column('port_name', String(1000)),\n",
    "    Column('terminal', String(100)),\n",
    "    Column('vessels', String(100), primary_key=True),\n",
    "    Column('vessels_code',String(100))\n",
    ")\n",
    "\n",
    "engine = create_engine('mysql+pymysql://root:rootinfo8988*Mines@localhost:3306/app', echo=True)\n",
    "metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9c292",
   "metadata": {},
   "source": [
    "Ajout de données depuis un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine('mysql+pymysql://root:rootinfo8988*Mines@localhost:3306/app', echo=True)\n",
    "\n",
    "# csv\n",
    "with engine.connect() as con:\n",
    "    with open('../msc_vrai.csv', 'r', encoding='utf-8') as file:\n",
    "        lecteur = csv.reader(file)\n",
    "        \n",
    "          \n",
    "\n",
    "        for ligne in lecteur:\n",
    "            print(ligne)\n",
    "            if len(ligne) < 7:\n",
    "                print(\"Ligne incomplète :\", ligne)\n",
    "                continue\n",
    "\n",
    "            name_carrier, code_route, name_route, port_code, port_name, terminal, vessels,vessels_code = ligne[0],ligne[1],ligne[2],ligne[3],ligne[4],ligne[5],ligne[6],ligne[7]\n",
    "\n",
    "            \n",
    "            query = text(\"\"\"\n",
    "                INSERT INTO data6 (name_carrier, code_route, name_route, port_code, port_name, terminal, vessels, vessels_code)\n",
    "                VALUES (:name_carrier, :code_route, :name_route, :port_code, :port_name, :terminal, :vessels, :vessels_code)\n",
    "            \"\"\")\n",
    "            con.execute(query, {\n",
    "                'name_carrier': name_carrier,\n",
    "                'code_route': code_route,\n",
    "                'name_route': name_route,\n",
    "                'port_code': port_code,\n",
    "                'port_name': port_name,\n",
    "                'terminal': terminal,\n",
    "                'vessels': vessels,\n",
    "                'vessels_code':vessels_code,\n",
    "            })\n",
    "\n",
    "    con.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
